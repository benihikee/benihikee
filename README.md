# ChatGPT

ChatGPT是生成型预训练变换模型（GPT），在GPT-3.5之上用基于人类反馈的监督学习和强化学习微调。这两种方法都用人类教练来提高模型性能，以人类干预增强机器学习效果，获得更逼真的结果。在监督学习的情况下为模型提供这样一些对话，在对话中教练充当用户和AI助理两种角色。在强化步骤中，人类教练首先为模型在先前对话中创建的响应评级。这些级别用于创建“奖励模型”，使用近端策略优化（PPO）的多次迭代来微调。这种策略优化算法比信任域策略优化（trust region policy optimization）算法更为高效。

此外，OpenAI继续从ChatGPT用户那里收集数据，这些数据可用于加强训练和微调ChatGPT。用户可对从ChatGPT收到的回复投赞成或反对票；投票时还可以额外填写文字回应。

关于ChatGPT编写和调试计算机程序的能力的训练，由于深度学习模型不懂编程，与所有其他基于深度学习的语言模型一样，只是在获取代码片段之间的统计相关性。

斯坦福大学的研究发现，GPT3已经可以解决70%的心智理论任务，相当于7岁儿童；至于GPT3.5（ChatGPT的同源模型），更是解决了93%的任务，心智相当于9岁儿童。但这并不意味着，ChatGPT就真正有心智理论。可能即使不将它设计到AI系统中，也可以作为“副产品”通过训练得到。因此，相比探究GPT3.5是不是真的有了心智还是像有心智，更需要反思的是这些测试本身。

